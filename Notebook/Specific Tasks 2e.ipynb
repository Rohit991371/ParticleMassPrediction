{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efca5c8-5637-4730-bc20-9d4cd0588e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9fb1b6f-c232-4768-b681-c094c36315d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb853727-be03-44e3-9eb9-0f52699a7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to load and preprocess data from parquet files\n",
    "# def load_data(file_paths):\n",
    "#     data_frames = []\n",
    "    \n",
    "#     for file_path in file_paths:\n",
    "#         print(f\"Loading file: {file_path}\")\n",
    "#         # Read parquet file\n",
    "#         df = pq.read_table(file_path).to_pandas()\n",
    "#         data_frames.append(df)\n",
    "    \n",
    "#     # Concatenate all data frames\n",
    "#     combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "#     print(f\"Combined dataframe shape: {combined_df.shape}\")\n",
    "    \n",
    "#     return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0d6d2-472e-4860-a6a9-daf57c97a7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dd4345e-1ea0-44f7-85cc-27dc364974cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['top_gun_opendata_3.parquet']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = glob.glob('*.parquet')\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b1491e9-9a4a-4224-9f31-b69541c2a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = glob.glob('*.parquet')\n",
    "    \n",
    "# if not file_paths:\n",
    "#     print(\"No parquet files found. Please provide the dataset files.\")\n",
    "    \n",
    "# print(f\"Found {len(file_paths)} parquet files: {file_paths}\")\n",
    "    \n",
    "#     # If there are more than 3 files, select a representative subset\n",
    "# if len(file_paths) > 3:\n",
    "#     # Select one from beginning, middle, and end for better representation\n",
    "#     indices = [0, len(file_paths) // 2, len(file_paths) - 1]\n",
    "#     selected_files = [file_paths[i] for i in indices]\n",
    "#     print(f\"Selected 3 representative files: {selected_files}\")\n",
    "# else:\n",
    "#     selected_files = file_paths\n",
    "    \n",
    "# # Load and preprocess data\n",
    "# df = load_data(selected_files)\n",
    "    \n",
    "# # Display dataset info\n",
    "# print(\"\\nDataset Information:\")\n",
    "# print(f\"Number of samples: {len(df)}\")\n",
    "# print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "# # Check for missing values\n",
    "# missing_values = df.isnull().sum().sum()\n",
    "# print(f\"Total missing values: {missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfecf313-b76d-43be-9226-374652c5641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to load data efficiently\n",
    "# def load_data_efficient(file_paths, use_columns=None, max_rows=10000):\n",
    "#     data_list = []\n",
    "#     total_rows_loaded = 0\n",
    "    \n",
    "#     for file in file_paths:\n",
    "#         print(f\"Loading {file} ...\")\n",
    "        \n",
    "#         # Load full dataset with only selected columns\n",
    "#         df = pd.read_parquet(file, columns=use_columns)\n",
    "#         # Limit rows to prevent memory crash\n",
    "#         if total_rows_loaded + len(df) > max_rows:\n",
    "#             remaining_rows = max_rows - total_rows_loaded\n",
    "#             df = df.iloc[:remaining_rows]\n",
    "#             data_list.append(df)\n",
    "#             break  # Stop if max_rows limit reached\n",
    "        \n",
    "#         data_list.append(df)\n",
    "#         total_rows_loaded += len(df)\n",
    "    \n",
    "#     df_final = pd.concat(data_list, ignore_index=True)\n",
    "#     return df_final\n",
    "\n",
    "# # Get parquet files\n",
    "# file_paths = glob.glob('*.parquet')\n",
    "\n",
    "# if not file_paths:\n",
    "#     print(\"No parquet files found. Please provide the dataset files.\")\n",
    "\n",
    "# print(f\"Found {len(file_paths)} parquet files: {file_paths}\")\n",
    "\n",
    "# # Select 3 files if more than 3 exist\n",
    "# if len(file_paths) > 3:\n",
    "#     indices = [0, len(file_paths) // 2, len(file_paths) - 1]\n",
    "#     selected_files = [file_paths[i] for i in indices]\n",
    "#     print(f\"Selected 3 representative files: {selected_files}\")\n",
    "# else:\n",
    "#     selected_files = file_paths\n",
    "\n",
    "# # Specify only needed columns\n",
    "# use_columns = [\"ieta\", \"iphi\", \"X_jet_Track_pT\", \"X_jet_ECAL\", \"am\"]  # Use only required features\n",
    "\n",
    "# # Load dataset efficiently (Limit to 100,000 rows max)\n",
    "# df = load_data_efficient(selected_files, use_columns=use_columns, max_rows=100000)\n",
    "\n",
    "# # Display dataset info\n",
    "# print(\"\\nDataset Information:\")\n",
    "# print(f\"Number of samples: {len(df)}\")\n",
    "# print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# # Check for missing values\n",
    "# missing_values = df.isnull().sum().sum()\n",
    "# print(f\"Total missing values: {missing_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8ced7f7-0162-4f5b-b77a-57f3e3d9728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 parquet files: ['top_gun_opendata_3.parquet']\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    file_path = glob.glob('*.parquet')\n",
    "    if not file_paths:\n",
    "        print(\"ERROR: No file found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(file_path)} parquet files: {file_paths}\")\n",
    "    for file in file_path:\n",
    "        df = pd.read_parquet(file)\n",
    "        print(f\"Shape : {df.shape}\")\n",
    "        print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd15429-bf8b-4921-9918-6bef22f4378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Extract target variable (particle mass)\n",
    "    y = df['am'].values\n",
    "    \n",
    "    # Extract image features (X_jet with the required channels)\n",
    "    # Assuming X_jet contains the channels in this order: [Track pT, DZ, D0, ECAL]\n",
    "    # used all channels as mentioned in the requirements (at least ECAL and Track pT)\n",
    "    \n",
    "    # First, let's identify the columns related to X_jet\n",
    "    x_jet_columns = [col for col in df.columns if 'X_jet' in col]\n",
    "    \n",
    "    # Print information about the features\n",
    "    print(f\"Number of X_jet columns: {len(x_jet_columns)}\")\n",
    "    print(f\"Sample X_jet column names: {x_jet_columns[:5] if len(x_jet_columns) >= 5 else x_jet_columns}\")\n",
    "    \n",
    "    # Reshape the data based on ieta, iphi dimensions and channels\n",
    "    # Assuming the data structure follows the 125x125 matrix with ieta, iphi coordinates\n",
    "    \n",
    "    # Get unique ieta and iphi values to determine dimensions\n",
    "    if 'ieta' in df.columns and 'iphi' in df.columns:\n",
    "        ieta_dim = len(df['ieta'].unique())\n",
    "        iphi_dim = len(df['iphi'].unique())\n",
    "        print(f\"Detected dimensions: ieta={ieta_dim}, iphi={iphi_dim}\")\n",
    "    else:\n",
    "        # Default to the mentioned 125x125 if coordinates are not explicitly in columns\n",
    "        ieta_dim, iphi_dim = 125, 125\n",
    "        print(f\"Using default dimensions: ieta={ieta_dim}, iphi={iphi_dim}\")\n",
    "    \n",
    "    # Extract X_jet data and reshape\n",
    "    # This part might need adjustment based on the actual structure of your data\n",
    "    try:\n",
    "        # Approach 1: If X_jet is stored as separate columns for each channel and position\n",
    "        if len(x_jet_columns) >= ieta_dim * iphi_dim * 4:  # 4 channels mentioned\n",
    "            X = np.zeros((len(df), ieta_dim, iphi_dim, 4))\n",
    "            \n",
    "            # Logic to reshape data from flat columns to 4D tensor\n",
    "            # This is a placeholder and needs to be adapted to your specific data format\n",
    "            print(\"Reshaping data from columns to 4D tensor...\")\n",
    "            \n",
    "        # Approach 2: If X_jet is already stored as a 4D tensor or can be easily reshaped\n",
    "        elif 'X_jet' in df.columns and isinstance(df['X_jet'].iloc[0], (np.ndarray, list)):\n",
    "            X = np.stack(df['X_jet'].values)\n",
    "            print(f\"Loaded X_jet as a tensor with shape: {X.shape}\")\n",
    "            \n",
    "        else:\n",
    "            # Another approach: If data is stored differently, adapt accordingly\n",
    "            print(\"Data format not recognized. Please adapt the preprocessing code.\")\n",
    "            # Placeholder for demonstration\n",
    "            X = np.random.rand(len(df), ieta_dim, iphi_dim, 4)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during data reshaping: {e}\")\n",
    "        print(\"Creating a placeholder tensor for demonstration purposes.\")\n",
    "        X = np.random.rand(len(df), ieta_dim, iphi_dim, 4)\n",
    "    \n",
    "    # Normalize the data\n",
    "    # For each channel separately\n",
    "    for i in range(X.shape[3]):\n",
    "        channel_data = X[:, :, :, i]\n",
    "        mean = np.mean(channel_data)\n",
    "        std = np.std(channel_data)\n",
    "        X[:, :, :, i] = (channel_data - mean) / (std + 1e-10)  # Add small epsilon to avoid division by zero\n",
    "    \n",
    "    print(f\"Processed data shape: X={X.shape}, y={y.shape}\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24636780-5a78-433c-8f08-7e34f705dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape, use_efficient_net=True):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    if use_efficient_net:\n",
    "        # Use EfficientNetB0 as base model (without top layers)\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        \n",
    "        # Make base model trainable\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        # If input has 4 channels but EfficientNet expects 3, adapt accordingly\n",
    "        if input_shape[2] != 3:\n",
    "            # Add a 1x1 convolution to adapt the number of channels\n",
    "            x = layers.Conv2D(3, (1, 1), padding='same')(inputs)\n",
    "            x = base_model(x)\n",
    "        else:\n",
    "            x = base_model(inputs)\n",
    "        \n",
    "        # Add pooling\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        \n",
    "    else:\n",
    "        # Custom CNN architecture\n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        \n",
    "        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        \n",
    "        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        \n",
    "        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Regression head\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer (no activation for regression)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01e9e28-fbe1-458a-819f-a3f3b328fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=100):\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        min_delta=0.0001\n",
    "    )\n",
    "    \n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c5fc7a-534a-47b0-ab14-f069267dc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Test Mean Absolute Error: {mae:.4f}\")\n",
    "    print(f\"Test RÂ² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
    "    plt.xlabel('Actual Mass')\n",
    "    plt.ylabel('Predicted Mass')\n",
    "    plt.title('Predicted vs Actual Particle Mass')\n",
    "    plt.savefig('pred_vs_actual.png')\n",
    "    \n",
    "    return mae, r2, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330e859-a752-4d3b-98bb-5cb7eae26032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Find all parquet files in the current directory\n",
    "    file_paths = glob.glob('*.parquet')\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(\"No parquet files found. Please provide the dataset files.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(file_paths)} parquet files: {file_paths}\")\n",
    "    \n",
    "    # If there are more than 3 files, select a representative subset\n",
    "    if len(file_paths) > 3:\n",
    "        # Select one from beginning, middle, and end for better representation\n",
    "        indices = [0, len(file_paths) // 2, len(file_paths) - 1]\n",
    "        selected_files = [file_paths[i] for i in indices]\n",
    "        print(f\"Selected 3 representative files: {selected_files}\")\n",
    "    else:\n",
    "        selected_files = file_paths\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_data(selected_files)\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(\"\\nDataset Information:\")\n",
    "    print(f\"Number of samples: {len(df)}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum().sum()\n",
    "    print(f\"Total missing values: {missing_values}\")\n",
    "    \n",
    "    # Extract features and target\n",
    "    X, y = preprocess_data(df)\n",
    "    \n",
    "    # Split the data into train and test sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Split train set into training and validation (80% of 80% = 64%, 20% of 80% = 16%)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"\\nData split sizes:\")\n",
    "    print(f\"Training: {X_train.shape[0]} samples ({X_train.shape[0]/len(df):.1%})\")\n",
    "    print(f\"Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(df):.1%})\")\n",
    "    print(f\"Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(df):.1%})\")\n",
    "    \n",
    "    # Build model\n",
    "    input_shape = X_train.shape[1:]\n",
    "    print(f\"\\nInput shape: {input_shape}\")\n",
    "    \n",
    "    # Decide which model to use\n",
    "    use_efficient_net = True  # Set to False to use custom CNN\n",
    "    model = build_cnn_model(input_shape, use_efficient_net=use_efficient_net)\n",
    "    \n",
    "    # Display model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    model, history = compile_and_train(model, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.plot(history.history['val_mae'])\n",
    "    plt.title('Model MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    \n",
    "    # Evaluate model\n",
    "    mae, r2, y_pred = evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save('particle_mass_regression_model.h5')\n",
    "    print(\"Model saved as 'particle_mass_regression_model.h5'\")\n",
    "    \n",
    "    # Return evaluation metrics\n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee864852-1cbc-4fc9-9ed7-6a92d3ea1832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e96963-4c7e-4e28-8b43-af7a95d76c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4c5e3-38ea-4dad-8aaf-cf8c7903d401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0db52-bcc5-4e89-a881-a847ef72af6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cfcec-46ef-4923-91f2-c0fadc75ac64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5869c55-0d27-40e3-aa3e-944f9d5e3e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a0e954-7008-4f1c-bed6-b72588731f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b2b64-bd76-4fff-b1ea-c919d7f1968d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bccce6-8651-45e7-a987-4e9dd69c84c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b7c8a-791f-41b4-8836-f7f0ebb5c98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b2ea6-13a6-4429-a06f-4dbb08dca93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a57f2f-2cba-4992-9fe3-620947b5f2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293d8bb-0cbe-4959-8bdb-2192a15f6dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b835aa-a78f-4ed6-b397-b88574923371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c382d-bf21-4f8d-934c-1fac1885dd63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1acd9-11e1-4261-a03d-27fba92a7a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce822b1d-4c90-484f-b0f7-4b89123cb96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b14ada-fe14-426b-8fe4-f549498ac350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f6c9e-c89c-4438-b7a2-4a17488ffa38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288caf75-fa34-4cbb-ab28-170e785cd1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba1397-0450-4d7d-89d9-e3cf8bd88720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c452ae-c36f-402d-be9d-b348fc1b04bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582979b-cc0c-487c-b461-810ee307e36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752544b3-0557-44db-84ad-b0935c59a33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43026383-8b16-43da-81cb-b02c35a35d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
